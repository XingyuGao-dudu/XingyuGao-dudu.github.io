
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>04. AI + P5.js Visual Music Site</title>
  <style>
    body {
      font-family: sans-serif;
      max-width: 960px;
      margin: auto;
      padding: 2rem;
      line-height: 1.7;
      background-color: #fff;
      color: #111;
    }
    h1 {
      font-size: 1.8rem;
      border-bottom: 1px solid #ccc;
      padding-bottom: 0.5rem;
    }
    h2 {
      color: #6a3cb3;
      margin-top: 2rem;
    }
    p, li {
      margin-bottom: 1rem;
    }
    img, iframe {
      max-width: 100%;
      margin: 1rem 0;
      border-radius: 4px;
    }
    .back {
      display: inline-block;
      margin-top: 2rem;
      padding: 0.5rem 1rem;
      background-color: #ffffff;
      color: #111;
      text-decoration: none;
      border-radius: 4px;
      box-shadow: 0 0 8px 0 rgba(180,138,255,0.5), 0 2px 4px 0 rgba(180,138,255,0.4);
    }
    a {
      color: #6a3cb3;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
  </style>
</head>
<body>

<h1>04. AI + P5.js Visual Music Site</h1>

<h2>Project Access</h2>
<ul>
  <li><strong>Sketch (hosted on Glitch):</strong> <a href="https://gainful-animated-jujube.glitch.me/" target="_blank">Open Website</a></li>
  <li><strong>GitHub Repository:</strong> <a href="https://git.arts.ac.uk/24011673/music-visualizer" target="_blank">GitHub Source Code</a></li>
  <li><strong>Video Demo:</strong> <a href="https://ual.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=a65143c7-558c-4100-af05-b2a500ef16e5" target="_blank">Panopto Video</a></li>
</ul>

<p><strong>Note:</strong> The Glitch page may go offline if the server is inactive. Please contact me if the demo becomes unavailable.</p>

<h2>Project Description</h2>
<p>
I created an interactive art website with two pages. On the first page, users upload a music file and enter a prompt. The system uses the Hugging Face AI model to generate an image, which is then pixelated and used as a visual background. On the second page, dynamic visual elements (waveform displays, rhythm bars) react in real-time to the music. The system explores the relationship between sound and imagery through AI and generative design.
</p>

<h2>Technical Overview</h2>
<p>
This project combines Node.js (Express) as backend and p5.js for frontend visual interactions. Music files are uploaded and processed using Multer. The server calls the Hugging Face API via Axios to generate imagery. On the frontend, p5.js handles audio FFT and Amplitude analysis. The pixelated background and responsive visual forms are created through custom canvas rendering and color sampling techniques.
</p>

<h2>Concept and Motivation</h2>
<p>
Inspired by On Kawara’s Today series, this work asks: “Can AI experience music like a human?” I explore how AI reshapes perception by transforming music into image and rhythm into motion. The work aims to reframe how we sense time and sound through computational methods. It challenges the assumption that sound and visuals must remain separate and reflects on how generative systems engage emotion and cognition.
</p>

<h2>References</h2>
<ul>
  <li>OpenProcessing visual inspiration: <a href="https://openprocessing.org/sketch/742194" target="_blank">Sketch Link</a></li>
</ul>

<a class="back" href="../index.html">← Back to Home</a>

</body>
</html>
