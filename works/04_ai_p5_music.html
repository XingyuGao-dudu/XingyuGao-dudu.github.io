<body>

<h1>04. AI + P5.js Visual Music Site</h1>

<!-- ====== üé¨ Video Section ====== -->
<h2>Project Video</h2>
<p>
<iframe src="https://drive.google.com/file/d/17-CtbUAHk3tQLUlTb1iyTzNGo_8Vyv34/preview" width="100%" height="480" allow="autoplay" frameborder="0"></iframe>

</p>
<!-- Â¶ÇÊûú‰ª•ÂêéÊúâÊú¨Âú∞ËßÜÈ¢ëÊñá‰ª∂ÔºåÂèØÁî® <video> Ê†áÁ≠æÊõøÊç¢‰∏äÈù¢ÁöÑË∂ÖÈìæÊé• -->
<!-- <video width="100%" controls>
  <source src="your_video_file.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video> -->

<!-- ====== üì∏ Images & GIFs Section ====== -->
<h2>Project Images & GIFs</h2>
<p>Selected stills and motion captures from the project.</p>

<img src="https://git.arts.ac.uk/24011673/CCI-Course-Notebook-2/assets/1284/70d26b79-5dce-457d-a40d-8a240e220ea2" alt="screenshot 1">
<img src="https://git.arts.ac.uk/24011673/CCI-Course-Notebook-2/assets/1284/0d274734-b85d-4764-b5c2-0d5e3c0ee600" alt="screenshot 2">
<img src="https://git.arts.ac.uk/24011673/CCI-Course-Notebook-2/assets/1284/a1785a96-644c-4542-a496-00087b1c1f9a" alt="screenshot 3">
<img src="https://git.arts.ac.uk/24011673/CCI-Course-Notebook-2/assets/1284/424a5247-3267-42da-ba24-63fb302de854" alt="screenshot 4">
<!-- Á§∫‰æãÂä®ÂõæÔºåÂèØËá™Ë°åÊõøÊç¢‰∏∫gifÈìæÊé• -->
<img src="https://git.arts.ac.uk/24011673/CCI-Course-Notebook-2/assets/1284/7ae12e96-a99c-4042-8286-79fe8a3722f7" alt="project gif">

<!-- ====== üìå Instructor Notes ====== -->
<h2>üìå Instructor Notes</h2>

<h3>(1) GitHub Repository</h3>
<p>
All code files have been uploaded to GitHub:  
<a href="https://git.arts.ac.uk/24011673/music-visualizer" target="_blank">View Repository</a>
</p>

<h3>(2) Hosting on Glitch</h3>
<p>
Since the project includes both frontend (p5.js) and backend (Node.js), GitHub Pages cannot host it directly.  
It is currently hosted on Glitch:
</p>
<img src="https://git.arts.ac.uk/24011673/CCI-Course-Notebook-2/assets/1284/6a378b49-d662-4150-a756-53af6b3694cd" alt="Glitch hosting">

<p><strong>‚ö† Note:</strong> Glitch may go offline if inactive. Please contact me if the demo becomes unavailable.</p>

<h3>(3) AI Image Generation Delay</h3>
<p>
AI-generated images may take time to appear due to API processing limits. Generating too many images might trigger rate limits.
</p>

<!-- ====== üåê Project Access ====== -->
<h2>Project Access</h2>
<ul>
  <li><strong>Sketch (hosted on Glitch):</strong> <a href="https://gainful-animated-jujube.glitch.me/" target="_blank">Open Website</a></li>
  <li><strong>GitHub Repository:</strong> <a href="https://git.arts.ac.uk/24011673/music-visualizer" target="_blank">GitHub Source Code</a></li>
  <li><strong>Video Demo:</strong> <a href="https://ual.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=a65143c7-558c-4100-af05-b2a500ef16e5" target="_blank">Panopto Video</a></li>
</ul>

<p><strong>Note:</strong> The Glitch page may go offline if the server is inactive. Please contact me if the demo becomes unavailable.</p>

<!-- ====== üìñ Project Description ====== -->
<h2>Project Description</h2>
<p>
I built an interactive art website with two pages. On the first page, users upload a music file and enter a text prompt. The system uses the Hugging Face AI model to generate an image based on the prompt, which is then pixelated as the visual background. On the second page, dynamic visual elements‚Äîsuch as waveform displays and rhythm bars‚Äîreact to the music in real time. The project explores the relationship between sound and imagery through AI and generative design.
</p>

<!-- ====== ‚öô Technical Overview ====== -->
<h2>Technical Overview</h2>
<p>
The system combines <strong>Node.js (Express)</strong> as backend and <strong>p5.js</strong> for frontend interactions.  
Music uploads are handled with <strong>Multer</strong>, and the server uses <strong>Axios</strong> to call the Hugging Face API for image generation.  
Frontend uses FFT and Amplitude analysis in p5.js to synchronize visuals with sound, while color sampling creates the pixelated background.
</p>

<!-- ====== üí° Concept ====== -->
<h2>Concept and Motivation</h2>
<p>
Inspired by On Kawara‚Äôs <em>Today</em> series, this work asks: <strong>Can AI experience music like a human?</strong>  
It reimagines how we perceive time and sound through computational media, challenging the assumption that sound and visuals must remain separate.  
The project reflects on how generative systems can mediate human emotion and cognition through algorithmic expression.
</p>

<!-- ====== üîó References ====== -->
<h2>References</h2>
<ul>
  <li>OpenProcessing visual inspiration: <a href="https://openprocessing.org/sketch/742194" target="_blank">Sketch Link</a></li>
</ul>

<a class="back" href="../index.html">‚Üê Back to Home</a>

</body>
